{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "head_001"},
   "source": [
    "# Wind Power Prediction - Complete Model Training & Export\n",
    "\n",
    "### Full Pipeline: Data Loading → Feature Engineering → Model Training → .pkl Export\n",
    "\n",
    "**Project:** Wind Power Prediction (Regression)\n",
    "\n",
    "**Task:** Predict hourly wind power output from meteorological features\n",
    "\n",
    "**Data Location:** `/content/drive/MyDrive/Suchitra/`\n",
    "\n",
    "**Output Files:** `model.pkl`, `scaler.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_001"},
   "source": [
    "## Step 0: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_001"},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Google Drive mounted successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_002"},
   "source": [
    "## Step 1: Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_002"},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn pandas numpy joblib\n",
    "print('Libraries installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_003"},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('All libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_003"},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_004"},
   "outputs": [],
   "source": [
    "train_path = '/content/drive/MyDrive/Suchitra/Train.csv'\n",
    "test_path = '/content/drive/MyDrive/Suchitra/Test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(f'Train Shape: {train_df.shape}')\n",
    "print(f'Test Shape: {test_df.shape}')\n",
    "print('\\nTrain Data:')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_004"},
   "source": [
    "## Step 3: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_005"},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "train_df = train_df.fillna(train_df.mean(numeric_only=True))\n",
    "test_df = test_df.fillna(test_df.mean(numeric_only=True))\n",
    "\n",
    "print(f'Missing in train: {train_df.isnull().sum().sum()}')\n",
    "print(f'Missing in test: {test_df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_005"},
   "source": [
    "## Step 4: Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_006"},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "cat_cols = train_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col != 'Time':\n",
    "        combined = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "        label_enc.fit(combined.astype(str))\n",
    "        train_df[col] = label_enc.transform(train_df[col].astype(str))\n",
    "        test_df[col] = label_enc.transform(test_df[col].astype(str))\n",
    "\n",
    "print(f'Encoded {len(cat_cols)} categorical columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_006"},
   "source": [
    "## Step 5: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_007"},
   "outputs": [],
   "source": [
    "train_df['Time'] = pd.to_datetime(train_df['Time'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "train_df['hour'] = train_df['Time'].dt.hour\n",
    "train_df['day'] = train_df['Time'].dt.day\n",
    "train_df['month'] = train_df['Time'].dt.month\n",
    "train_df['dayofweek'] = train_df['Time'].dt.dayofweek\n",
    "train_df['is_weekend'] = train_df['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "print('Temporal features created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_008"},
   "outputs": [],
   "source": [
    "for col in ['WD_10m', 'WD_100m']:\n",
    "    train_df[col + '_sin'] = np.sin(np.deg2rad(train_df[col]))\n",
    "    train_df[col + '_cos'] = np.cos(np.deg2rad(train_df[col]))\n",
    "\n",
    "print('Circular encoding for wind direction created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_009"},
   "outputs": [],
   "source": [
    "for col in ['WS_10m', 'WS_100m']:\n",
    "    train_df[col + '_sq'] = train_df[col] ** 2\n",
    "    train_df[col + '_cu'] = train_df[col] ** 3\n",
    "\n",
    "print('Polynomial wind speed features created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_010"},
   "outputs": [],
   "source": [
    "train_df['temp_humidity'] = train_df['Temp_2m'] * train_df['RelHum_2m']\n",
    "train_df['temp_dew_diff'] = train_df['Temp_2m'] - train_df['DP_2m']\n",
    "train_df['wind_shear'] = train_df['WS_100m'] - train_df['WS_10m']\n",
    "\n",
    "print(f'Interaction features created')\n",
    "print(f'Total features: {train_df.shape[1] - 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_007"},
   "source": [
    "## Step 6: Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_011"},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['Power', 'Time'])\n",
    "y_train = train_df['Power']\n",
    "\n",
    "if 'Time' in test_df.columns:\n",
    "    X_test = test_df.drop(columns=['Time'])\n",
    "else:\n",
    "    X_test = test_df.copy()\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_008"},
   "source": [
    "## Step 7: Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_012"},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Features scaled successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_009"},
   "source": [
    "## Step 8: Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_013"},
   "outputs": [],
   "source": [
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train set: {X_train_split.shape[0]} samples')\n",
    "print(f'Validation set: {X_val_split.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_010"},
   "source": [
    "## Step 9: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_014"},
   "outputs": [],
   "source": [
    "print('Training Random Forest model...\\n')\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train_split, y_train_split)\n",
    "print('\\nModel training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_011"},
   "source": [
    "## Step 10: Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_015"},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val_split)\n",
    "\n",
    "mae = mean_absolute_error(y_val_split, y_val_pred)\n",
    "mse = mean_squared_error(y_val_split, y_val_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val_split, y_val_pred)\n",
    "\n",
    "print('MODEL PERFORMANCE METRICS')\n",
    "print('='*50)\n",
    "print(f'MAE: {mae:.6f}')\n",
    "print(f'RMSE: {rmse:.6f}')\n",
    "print(f'R2 Score: {r2:.6f}')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_012"},
   "source": [
    "## Step 11: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_016"},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('Top 10 Important Features:')\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_013"},
   "source": [
    "## Step 12: Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_017"},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('First 10 predictions:')\n",
    "print(y_test_pred[:10])\n",
    "print(f'\\nPrediction Min: {y_test_pred.min():.6f}')\n",
    "print(f'Prediction Max: {y_test_pred.max():.6f}')\n",
    "print(f'Prediction Mean: {y_test_pred.mean():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_014"},
   "source": [
    "## Step 13: Save Predictions CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_018"},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'ID': range(1, len(y_test_pred) + 1),\n",
    "    'Predicted_Power': y_test_pred\n",
    "})\n",
    "\n",
    "output_path = '/content/drive/MyDrive/Suchitra/Predicted_Power_Output.csv'\n",
    "output_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f'Predictions saved to: {output_path}')\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_015"},
   "source": [
    "## Step 14: Serialize Model and Scaler (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_019"},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = '/content/drive/MyDrive/Suchitra/model.pkl'\n",
    "scaler_path = '/content/drive/MyDrive/Suchitra/scaler.pkl'\n",
    "\n",
    "joblib.dump(model, model_path)\n",
    "print(f'Model saved: {model_path}')\n",
    "\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f'Scaler saved: {scaler_path}')\n",
    "\n",
    "model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "scaler_size = os.path.getsize(scaler_path) / (1024)\n",
    "\n",
    "print(f'\\nModel size: {model_size:.2f} MB')\n",
    "print(f'Scaler size: {scaler_size:.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_016"},
   "source": [
    "## Step 15: Verify Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_020"},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "\n",
    "print('Model loaded successfully')\n",
    "\n",
    "sample_test = X_test_scaled[:1]\n",
    "sample_pred = loaded_model.predict(sample_test)\n",
    "\n",
    "print(f'Test prediction: {sample_pred[0]:.6f}')\n",
    "print(f'Original: {y_test_pred[0]:.6f}')\n",
    "print(f'\\nModel working correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "section_017"},
   "source": [
    "## Step 16: Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "cell_021"},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('WIND POWER PREDICTION - COMPLETE')\n",
    "print('='*60)\n",
    "\n",
    "print('\\nPROJECT SUMMARY:')\n",
    "print(f'Training Samples: {X_train_split.shape[0]:,}')\n",
    "print(f'Validation Samples: {X_val_split.shape[0]:,}')\n",
    "print(f'Test Samples: {X_test.shape[0]:,}')\n",
    "print(f'Total Features: {X_train.shape[1]}')\n",
    "\n",
    "print('\\nMODEL DETAILS:')\n",
    "print(f'Algorithm: Random Forest (300 trees)')\n",
    "\n",
    "print('\\nPERFORMANCE METRICS:')\n",
    "print(f'MAE: {mae:.6f}')\n",
    "print(f'RMSE: {rmse:.6f}')\n",
    "print(f'R2 Score: {r2:.6f}')\n",
    "\n",
    "print('\\nFILES SAVED:')\n",
    "print(f'Model: {model_path}')\n",
    "print(f'Scaler: {scaler_path}')\n",
    "print(f'Predictions: {output_path}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('STATUS: READY FOR DEPLOYMENT')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {"provenance": []},
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}